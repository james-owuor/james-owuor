{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b08b761-028c-4662-b53d-6d657c28d2c0",
   "metadata": {},
   "source": [
    "#### Resampling Methods\n",
    "##### Involves repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional info about the fitted model.\n",
    "##### Two most common resampling methods: ***Cross-validation*** and the ***Bootstrap***\n",
    "##### Cross-validation can be used to estimate the ***test error*** associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility.\n",
    "##### The process of evaluating a model's performance is known as ***model assessment***\n",
    "##### ***The Validation Set Approach:***\n",
    "##### --Involves randomly dividing the available set of observations ito two parts : ***training set***, and ***validation set or hold-out set***\n",
    "##### The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate is typically assessed using ***MSE** in the case of a quantitative response.\n",
    "##### ***Leave-One-Out Cross-Validation*** \n",
    "##### ***k-Fold Cross-Validation*** --randomly dividing the set observations into k groups or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining (k-1) folds.\n",
    "##### ***The Bootstrap*** --can be used to quantify the uncertainity associated with a given estimator or statistical learning method.\n",
    "##### The bootstrap can be used to estimate the ***standard errors*** of the coefficients from a linear regression fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "59db13c8-36ea-4456-ba48-b22baaab42d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\colli\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\colli\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\colli\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\colli\\anaconda3\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\colli\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from statsmodels) (1.11.4)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: six in c:\\users\\colli\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ca34c4a1-ec29-478d-84ab-ee3471c57b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ISLP in c:\\users\\colli\\anaconda3\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.7.1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from ISLP) (1.26.4)\n",
      "Requirement already satisfied: scipy>=0.9 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from ISLP) (1.11.4)\n",
      "Requirement already satisfied: pandas>=0.20 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from ISLP) (2.1.4)\n",
      "Requirement already satisfied: lxml in c:\\users\\colli\\appdata\\roaming\\python\\python311\\site-packages (from ISLP) (4.9.3)\n",
      "Requirement already satisfied: scikit-learn>=1.2 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from ISLP) (1.2.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\colli\\anaconda3\\lib\\site-packages (from ISLP) (1.2.0)\n",
      "Requirement already satisfied: statsmodels>=0.13 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from ISLP) (0.14.0)\n",
      "Requirement already satisfied: lifelines in c:\\users\\colli\\anaconda3\\lib\\site-packages (from ISLP) (0.30.0)\n",
      "Requirement already satisfied: pygam in c:\\users\\colli\\anaconda3\\lib\\site-packages (from ISLP) (0.9.1)\n",
      "Requirement already satisfied: torch in c:\\users\\colli\\anaconda3\\lib\\site-packages (from ISLP) (2.7.1)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\colli\\anaconda3\\lib\\site-packages (from ISLP) (2.5.2)\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\colli\\anaconda3\\lib\\site-packages (from ISLP) (1.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from pandas>=0.20->ISLP) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from pandas>=0.20->ISLP) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from pandas>=0.20->ISLP) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from scikit-learn>=1.2->ISLP) (2.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from statsmodels>=0.13->ISLP) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\colli\\appdata\\roaming\\python\\python311\\site-packages (from statsmodels>=0.13->ISLP) (23.2)\n",
      "Requirement already satisfied: matplotlib>=3.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from lifelines->ISLP) (3.8.0)\n",
      "Requirement already satisfied: autograd>=1.5 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from lifelines->ISLP) (1.8.0)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from lifelines->ISLP) (0.5.0)\n",
      "Requirement already satisfied: formulaic>=0.2.2 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from lifelines->ISLP) (1.1.1)\n",
      "Requirement already satisfied: progressbar2<5.0.0,>=4.2.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from pygam->ISLP) (4.5.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from pytorch-lightning->ISLP) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from pytorch-lightning->ISLP) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from pytorch-lightning->ISLP) (4.12.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from pytorch-lightning->ISLP) (0.14.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\colli\\anaconda3\\lib\\site-packages (from torch->ISLP) (3.13.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from torch->ISLP) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\colli\\anaconda3\\lib\\site-packages (from torch->ISLP) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from torch->ISLP) (3.1.3)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from formulaic>=0.2.2->lifelines->ISLP) (1.3.0)\n",
      "Requirement already satisfied: wrapt>=1.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from formulaic>=0.2.2->lifelines->ISLP) (1.14.1)\n",
      "Requirement already satisfied: requests in c:\\users\\colli\\appdata\\roaming\\python\\python311\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (3.9.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\colli\\anaconda3\\lib\\site-packages (from lightning-utilities>=0.10.0->pytorch-lightning->ISLP) (68.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines->ISLP) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines->ISLP) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines->ISLP) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines->ISLP) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines->ISLP) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from matplotlib>=3.0->lifelines->ISLP) (3.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\colli\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels>=0.13->ISLP) (1.16.0)\n",
      "Requirement already satisfied: python-utils>=3.8.1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from progressbar2<5.0.0,>=4.2.0->pygam->ISLP) (3.9.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch->ISLP) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\colli\\anaconda3\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning->ISLP) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from jinja2->torch->ISLP) (2.1.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\colli\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\colli\\appdata\\roaming\\python\\python311\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\colli\\appdata\\roaming\\python\\python311\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\colli\\appdata\\roaming\\python\\python311\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\colli\\appdata\\roaming\\python\\python311\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install ISLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9c3d1d4c-8742-4387-801f-a015f5052e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from ISLP import load_data\n",
    "from ISLP.models import (ModelSpec as MS, summarize, poly)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "from sklearn.model_selection import (cross_validate, KFold, ShuffleSplit)\n",
    "from sklearn.base import clone\n",
    "from ISLP.models import sklearn_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f14581-0c90-4405-9688-4c3c55b2191c",
   "metadata": {},
   "source": [
    "#### The Validation Set Approach\n",
    "##### We explore the use of the validation set approach in order to estimate the test error rates that result from fitting various linear models on the Auto data set.\n",
    "##### we have 396 observations , we split into two equal sets of size 196 \n",
    "##### set a random seed so that results obtained can be reproduced precisely ata later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "9dd7bb28-f98b-4ecd-86f1-a60b47bd848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Auto= load_data(\"Auto\")\n",
    "Auto_train, Auto_valid = train_test_split(Auto, test_size=196, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7a8ac6d7-4130-4adb-b74c-cc30c094e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we fit a linear regression using only the observations corresponding to the training set Auto_train\n",
    "hp_mm= MS([\"horsepower\"])\n",
    "X_train= hp_mm.fit_transform(Auto_train)\n",
    "y_train= Auto_train[\"mpg\"]\n",
    "model = sm.OLS(y_train, X_train)\n",
    "results= model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "80976d21-c6a2-4108-8818-6f955bc40401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.616617069669882"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We now use the predict() method of results evaluated on the model matrix for this model created using teh validation data set.\n",
    "X_valid= hp_mm.transform(Auto_valid)\n",
    "y_valid= Auto_valid[\"mpg\"]\n",
    "valid_pred= results.predict(X_valid)\n",
    "np.mean((y_valid - valid_pred)**2) #MSE of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4397c533-a993-499c-baf1-1da641371ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WE can also estimate the validation error for higher-degree ploynomial regressions\n",
    "# we first provide a function evalMSE() that takes a model string as well as training and test set and returns the MSE on the test set\n",
    "def evalMSE(terms, response, train, test):\n",
    "    mm= MS(terms)\n",
    "    X_train= mm.fit_transform(train)\n",
    "    y_train = train[response]\n",
    "\n",
    "    X_test = mm.transform(test)\n",
    "    y_test = test[response]\n",
    "    results= sm.OLS(y_train, X_train).fit()\n",
    "    test_pred= results.predict(X_test)\n",
    "    return np.mean((y_test - test_pred)**2) #MSE of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0d17166a-6b41-4f17-b0c6-1b331daa1483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707, 18.76303135, 18.79694163])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets use this function to estimate the validation MSE using linear, quadratic and cubic fits.\n",
    "#we use enumerate() which gives both the values and indices of objects as one iterates over a loop\n",
    "MSE = np.zeros(3) #creates an array to store results\n",
    "for idx, degree in enumerate(range(1,4)): #the loop iterates over polynomial degrees 1-3 and updates MSE\n",
    "    MSE[idx] = evalMSE([poly(\"horsepower\", degree)], \"mpg\", Auto_train, Auto_valid)\n",
    "MSE    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "5b9c207b-4b74-4c53-b019-3447c1ce5df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.75540796, 16.94510676, 16.97437833])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If we choose different training/validation split instead, we can expect different errors on the validation set\n",
    "Auto_train, Auto_valid = train_test_split(Auto, test_size=196, random_state=3)\n",
    "MSE = np.zeros(3)\n",
    "for idx, degree in enumerate(range(1,4)):\n",
    "    MSE[idx] = evalMSE([poly(\"horsepower\", degree)], \"mpg\", Auto_train, Auto_valid)\n",
    "MSE  \n",
    "\n",
    "###  A model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower, and there is no evidence of an improvement in using a cubic function of horsepower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d33691-a6d1-4405-8e10-5fa7b242d12e",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573cd58-afae-4a0d-b8ed-40bcfa5b3163",
   "metadata": {},
   "source": [
    "##### What is a wrapper?\n",
    "##### Its like a translator or adapter that lets two incompatible tools work together\n",
    "##### Example For task A : you fit a model using statsmodels(e.g logistic regression) and task B you want to use scikit-learn's cross-validation tools.\n",
    "##### Problem: statsmodels and scikit-learn don't natively understand each other's formats\n",
    "##### Therefore sklearn_sm() wrapper acts as the adapter\n",
    "##### ***Key components of sklearn_sm()***\n",
    "##### Input: a ***statsmodels*** model(e.g, logit)\n",
    "##### Optional Arguments: ***model_str***: formula(e.g \"mpg~ horsepower + weight\").\n",
    "#####                     ***model_args***: e.g {\"family\":sm.families.Binomial() for       logistic regression}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "22951f14-e993-48f3-8775-9aa5d81ab65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just as an illustration\n",
    "#from ISLP import sklearn_sm\n",
    "#import statsmodels.api as sm\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#step1: Define the wrapper\n",
    "#logit_wrapper= sklearn_sm(\n",
    " #   sm.Logit,  #statsmodels model\n",
    "  #  model_str= \"mpg01~ horsepower + weight\", #formula\n",
    "   # model_args={\"family\": sm.families.Binomial()} #Logistic regression\n",
    "#)\n",
    "\n",
    "#step2: Use with scikit_learn's cross_validation\n",
    "#scores = cross_val_score(\n",
    " #   logit_wrapper,  #wrapped model\n",
    "  #  Auto,  #Data\n",
    "   # Auto[\"mpg01\"], #target\n",
    "    #cv=5    #5-fold cross_validation\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5787b1-eb88-44bd-aa21-bb3652c7dc90",
   "metadata": {},
   "source": [
    "##### Here is our wrapper in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "54700fa4-eeed-44d5-afae-519ea17fa115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.231513517929233"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_model = sklearn_sm(sm.OLS, MS([\"horsepower\"]))\n",
    "X, Y = Auto.drop(columns=[\"mpg\"]), Auto[\"mpg\"]\n",
    "cv_results = cross_validate(hp_model, X, Y, cv=Auto.shape[0])\n",
    "cv_err = np.mean(cv_results[\"test_score\"])\n",
    "cv_err\n",
    "\n",
    "# hp_model : an object with appropriate fit(), predict() and score()\n",
    "# cv : specifies an integer K results in K-fold cross-validation\n",
    "# cv=Auto.shape[0] : provides value corresponding to the total number of observations, \n",
    "# which results in leave-one-out cross-validation(LOOCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596df37-b902-4d07-b6ae-529aa1b01ca1",
   "metadata": {},
   "source": [
    "##### We can repeat the procedure for increasingly complex polynomial fit\n",
    "##### To automate the process, we again use a for loop which iteratively fits polynomial regressions of degree 1 to 5, computes the associated cross-validation error, and stores it in the ith element of the vector cv_error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d3ff329c-8dc2-44c3-9b89-c0bcbbf46aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.23151352, 19.24821312, 19.33498406, 19.4244303 , 19.0332262 ])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "H= np.array(Auto[\"horsepower\"]) #array of horsepower values\n",
    "M = sklearn_sm(sm.OLS)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X = np.power.outer(H, np.arange(d + 1)) #creates polynomial features up to degree d for the horse values: computes element_wise power operations to create the design matrix\n",
    "    M_CV = cross_validate(M, X, Y, cv=Auto.shape[0]) #LOOCV\n",
    "    cv_error[i] = np.mean(M_CV[\"test_score\"]) #average MSE\n",
    "cv_error    \n",
    "\n",
    "## We see a sharp drop in the estimated test MSE between linear models and quadratic fits, but then no clear improvement using higher-degree polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d411c5-7cd7-46f7-af7f-958fd2068ca3",
   "metadata": {},
   "source": [
    "##### In the CV example above we used K = n , we can also use K < n. Here we use KFold() to partition the data into K = 10 random groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "dfafbec2-c1ec-44dd-9b25-9630aa89d5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.20766449, 19.18533142, 19.27626666, 19.47848402, 19.13719075])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_error = np.zeros(5)\n",
    "cv= KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "for i, d in enumerate(range(1,6)):\n",
    "    X= np.power.outer(H, np.arange(d + 1))\n",
    "    M_CV = cross_validate(M, X, Y, cv=cv)\n",
    "    cv_error[i] = np.mean(M_CV[\"test_score\"])\n",
    "cv_error    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd5dfa-c814-40f2-8c33-5910cd3dc0b3",
   "metadata": {},
   "source": [
    "##### The cross-validate() function is flexible and can take different splitting mechanisms as an argument.\n",
    "##### For instance, one can use ShuffleSplit() function to implement the validation set approach just as easily as K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7d51525c-adcc-446d-88fd-5b7e3273e387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.61661707])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=1,\n",
    "                         test_size=196,\n",
    "                         random_state=0)\n",
    "results= cross_validate(hp_model,\n",
    "                       Auto.drop([\"mpg\"], axis=1),\n",
    "                       Auto[\"mpg\"],\n",
    "                       cv= validation)\n",
    "results[\"test_score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a8ea6-8dfb-46e9-8339-271b4d44c5f2",
   "metadata": {},
   "source": [
    "##### Lets estimate the variability in the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "9056fda9-d8a5-473d-80d4-0b136978eff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.802232661034164, 1.4218450941091862)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation = ShuffleSplit(n_splits=10,\n",
    "                         test_size=196,\n",
    "                         random_state=0) #randomly splits data multiple times for robust validation\n",
    "results = cross_validate(hp_model,\n",
    "                        Auto.drop([\"mpg\"], axis=1),\n",
    "                        Auto[\"mpg\"],\n",
    "                        cv=validation)\n",
    "results[\"test_score\"].mean(), results[\"test_score\"].std()\n",
    "\n",
    "#Note that this SD is not a valid estimate of the sampling variability of the mean test score or the individual scores, since the randomly-selected training samples overlap and hence introduce correlations.\n",
    "# But it does give an idea of the Monte Carlo variation incurred by picking different random folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850090bc-133d-4619-8dc4-7b00a1ac2d5d",
   "metadata": {},
   "source": [
    "### The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dbf7f9-c3f4-4f3b-b249-f8d86bf26f98",
   "metadata": {},
   "source": [
    "##### Estimating the Accuracy of a Statistic of Interest\n",
    "\n",
    "##### We use the Portfolio data set in the ISLP package .The goal is to estimate the sampling variance of the parameter (coefficients).\n",
    "##### We will create a function alpha_func(), which takes as input a dataframe D assumed to have columns X and Y, as well as vector idx indicating which observations should be used to estimate the weight(coefficient). The function then outputs the estimate for (coefficients) based on the selected observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "006f0f2a-1433-4647-b449-d5efc6a14d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Portfolio = load_data(\"Portfolio\")\n",
    "def alpha_func(D, idx):\n",
    "    cov_ = np.cov(D[[\"X\", \"Y\"]].loc[idx], rowvar=False)\n",
    "    return ((cov_[1,1] - cov_[0,1])/(cov_[0,0] + cov_[1,1] - 2*cov_[0,1]))\n",
    "# assume the Portfolio has two columns: \n",
    "# X: returns of asset X\n",
    "# Y: returns of asset Y\n",
    "# the alpha_func computes an optimal portfolio weight(α) that miinimizes variance\n",
    "#The function calculates the cov_: covariance of matrix of X and Y for the resampled data (idx)\n",
    "# The formula gives the weight (α) to invest in X for minimum portfolio risk: α = [Var(Y) - Cov(X,Y)] / [Var(X) + Var(Y) - 2*Cov(X,Y)]\n",
    "\n",
    "            # Set up bootstrap\n",
    "#n_bootstraps = 1000\n",
    "#alpha_estimates = np.zeros(n_bootstraps)\n",
    "\n",
    "#for i in range(n_bootstraps):\n",
    "    # Resample indices with replacement\n",
    " #   idx = np.random.choice(len(Portfolio), len(Portfolio), replace=True)\n",
    "    # Compute alpha for this resample\n",
    "  #  alpha_estimates[i] = alpha_func(Portfolio, idx)\n",
    "\n",
    "# Results\n",
    "#print(f\"Mean α: {np.mean(alpha_estimates):.3f}\")\n",
    "#print(f\"95% CI: [{np.percentile(alpha_estimates, 2.5):.3f}, {np.percentile(alpha_estimates, 97.5):.3f}]\")\n",
    "#print(f\"95% CI: [{np.percentile(alpha_estimates, 2.5):.3f}, {np.percentile(alpha_estimates, 97.5):.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "63b29029-40aa-4ad2-9202-583bacb7a0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57583207459283"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets estimate (α) using all 100 observations\n",
    "alpha_func(Portfolio, range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ad8cecd2-0f07-4fd5-887d-033570d8a8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6074452469619004"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NExt we randomly select 100 observations from range(100), with replacement.\n",
    "#This is equivalent to constructing a new bootstrap data set and recomputing alpha based on new data\n",
    "rng = np.random.default_rng(0) #creates random  number generator object with a fixed seed(0) for reproducibility\n",
    "alpha_func(Portfolio, \n",
    "          rng.choice(100,\n",
    "                    100,\n",
    "                    replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "efec1aa5-d70d-4b0a-b695-e9eb091c5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This process can be generalized to create a simple function boot_SE() for computing the bootstrap standard error for arbitrary functions that take only a data frame as an argument  \n",
    "\n",
    "def boot_SE(func,\n",
    "          D,\n",
    "          n=None,\n",
    "          B=1000,\n",
    "          seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    first_, second_ = 0, 0\n",
    "    n = n if n is not None else D.shape[0] #/ n or D.shape[0]\n",
    "    for _ in range(B):\n",
    "        idx = rng.choice(D.index,\n",
    "                        n,\n",
    "                        replace=True)\n",
    "        value = func(D, idx)\n",
    "        first_ += value\n",
    "        second_ += value**2\n",
    "    return np.sqrt(second_ / B - (first_ / B) **2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "592aff6a-3c0b-4a33-abf6-92b4773beab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09118176521277699"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets use our function to evaluate the accuracy of our estimate of alpha using B = 1,000 bootstrap replications\n",
    "alpha_SE= boot_SE(alpha_func,\n",
    "                 Portfolio,\n",
    "                 B= 1000,\n",
    "                 seed=0)\n",
    "alpha_SE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d5338-63cb-4be9-bcdd-9781ab7e1b1f",
   "metadata": {},
   "source": [
    "#### Estimating the Accuracy of a Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e1b14b-89e7-4c39-97d7-f287fa562310",
   "metadata": {},
   "source": [
    "##### The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. \n",
    "##### Here we use the bootstrap approach to assess the variability of the estimates for B0 and B1,the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "27ef88e4-e4f3-4076-8e89-d36768b279ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit an OLS regression on a bootstrap sample of the data\n",
    "def boot_OLS(model_matrix, response, D, idx):\n",
    "    D_= D.loc[idx]\n",
    "    Y_= D_[response]\n",
    "    X_= clone(model_matrix).fit_transform(D_)\n",
    "    return sm.OLS(Y_, X_).fit().params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "971ad25a-ebce-49bf-969e-fb0f6f83c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partially applied version of boot_OLS : fixes model_matrix=MS([\"horsepower\"]) and response= \"mpg\"\n",
    "#Now only requires D and idx to run\n",
    "hp_func = partial(boot_OLS, MS([\"horsepower\"]), \"mpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1e4a0afe-f95b-4f32-8bba-ad65c0d2a798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.12226577, -0.1555926 ],\n",
       "       [37.18648613, -0.13915813],\n",
       "       [37.46989244, -0.14112749],\n",
       "       [38.56723252, -0.14830116],\n",
       "       [38.95495707, -0.15315141],\n",
       "       [39.12563927, -0.15261044],\n",
       "       [38.45763251, -0.14767251],\n",
       "       [38.43372587, -0.15019447],\n",
       "       [37.87581142, -0.1409544 ],\n",
       "       [37.95949036, -0.1451333 ]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the hp_func() can now be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement.\n",
    "#We first demo its utility on 10 bootstrap samples\n",
    "rng = np.random.default_rng(0)\n",
    "#get the actual indices from the Auto dataframe\n",
    "indices = Auto.index.tolist()\n",
    "np.array([hp_func(Auto,\n",
    "                 rng.choice(indices, len(indices), replace=True)) for _ in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6c48f3eb-5bab-44e8-8df1-478c4d77109f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.731176\n",
       "horsepower    0.006092\n",
       "dtype: float64"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We use boot_SE() to compute the standard errors of 1000 bootstrap estimates for the intercept and slope terms\n",
    "hp_se = boot_SE(hp_func, Auto, B=1000, seed=10)\n",
    "hp_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "dfd760b7-4cc8-4447-952f-3e73b33edb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept     0.717\n",
       "horsepower    0.006\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#standard formulas can be used to compute the standard errors for the regression coefficients in a linear model\n",
    "hp_model.fit(Auto, Auto[\"mpg\"])\n",
    "model_se = summarize(hp_model.results_)[\"std err\"]\n",
    "model_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ee13ec8b-1e51-4060-b3d0-49291edc0c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  1.538641\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.024696\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000090\n",
       "dtype: float64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting the quadratic model to the data\n",
    "quad_model = MS([poly(\"horsepower\", 2, raw=True)])\n",
    "quad_func= partial(boot_OLS, quad_model, \"mpg\")\n",
    "boot_SE(quad_func, Auto, B=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d657eb38-04df-407f-b985-5c5abef18f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept                                  1.800\n",
       "poly(horsepower, degree=2, raw=True)[0]    0.031\n",
       "poly(horsepower, degree=2, raw=True)[1]    0.000\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we compare the results to the SE computed using sm.OLS()\n",
    "M= sm.OLS(Auto[\"mpg\"], quad_model.fit_transform(Auto))\n",
    "summarize(M.fit())[\"std err\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d3a64-9145-4260-8973-a764f01a0a12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58b0c277-c2ae-4f87-a5fa-59ed93627a86",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "##### In Classification chapter we used logistic regression to predict the probability of default using ***income*** and ***balance*** on ***Default*** data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning the analysis.\n",
    "\n",
    "##### a). Fit a logistic regression model that uses income and balance to predict default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "311dc5a9-bfd3-4ad3-a29e-e6e408104a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>-11.540500</td>\n",
       "      <td>0.435000</td>\n",
       "      <td>-26.544</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balance</th>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.835</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income</th>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>4.174</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                coef   std err       z  P>|z|\n",
       "intercept -11.540500  0.435000 -26.544    0.0\n",
       "balance     0.005600  0.000000  24.835    0.0\n",
       "income      0.000021  0.000005   4.174    0.0"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Default = load_data(\"Default\")\n",
    "dropVar=Default.columns.drop([\"default\", \"student\"])\n",
    "y= Default.default == \"Yes\"\n",
    "X = MS(dropVar).fit_transform(Default)\n",
    "model= sm.GLM(y,X, family = sm.families.Binomial())\n",
    "results= model.fit()\n",
    "summarize(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3fa49c-e0ed-4d7c-87ff-72449f063565",
   "metadata": {},
   "source": [
    "##### b). Using the validation set approach, estimate the test error of this model. Inorder to do this, you must perform the following steps:\n",
    "\n",
    "##### ***i***. Split the sample set into a training set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "76ec6e28-feef-484b-a10e-cdc2dc17cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_train, default_valid = train_test_split(Default, test_size=5000, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f97ed-68e5-4f5c-9eae-5900eed423f7",
   "metadata": {},
   "source": [
    "##### ***ii*** Fit a multiple regression model using only the training observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "41203560-eaad-4522-8b1b-d040dd7baeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>-11.389600</td>\n",
       "      <td>0.635000</td>\n",
       "      <td>-17.935</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>balance</th>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.792</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>2.151</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                coef   std err       z  P>|z|\n",
       "intercept -11.389600  0.635000 -17.935  0.000\n",
       "balance     0.005600  0.000000  16.792  0.000\n",
       "income      0.000016  0.000007   2.151  0.031"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=MS(dropVar)\n",
    "X_train = var.fit_transform(default_train)\n",
    "y_train= default_train[\"default\"]==\"Yes\"\n",
    "model= sm.GLM(y_train, X_train, family= sm.families.Binomial())\n",
    "results= model.fit()\n",
    "summarize(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a66b5f-2147-42f1-b204-7483f34c5a9b",
   "metadata": {},
   "source": [
    "##### ***iii*** Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c9f85c17-6a83-418f-9bc8-83796a328ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9394    False\n",
       "898     False\n",
       "2398    False\n",
       "5906    False\n",
       "2343    False\n",
       "        ...  \n",
       "3996    False\n",
       "5889    False\n",
       "4577    False\n",
       "8600    False\n",
       "847     False\n",
       "Length: 5000, dtype: bool"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess the validation data using the same transformer var\n",
    "X_valid = var.transform(default_valid)\n",
    "y_valid = default_valid[\"default\"]== \"Yes\"\n",
    "valid_pred = results.predict(X_valid)\n",
    "valid_pred_class = (valid_pred>0.5)\n",
    "valid_pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb83a14-f892-4683-9d17-99ff1b3e6724",
   "metadata": {},
   "source": [
    "##### ***iv*** Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b00c0226-fa8f-4690-8e93-c26af9c68cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Truth</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>4801</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>132</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Truth      False  True \n",
       "Predicted              \n",
       "False       4801     13\n",
       "True         132     54"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ISLP import confusion_table\n",
    "confusion_table(y_valid, valid_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "48a97b7d-c54e-475c-9ea0-bfbb80ec4e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the set error\n",
    "np.mean(y_valid!=valid_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528e360-c1d1-4756-8ede-56d8aa418dcf",
   "metadata": {},
   "source": [
    "##### c) Repeat the process in (b) three times, using three diffrent splits of the observations into a training set and a validation set. comment on the results obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "585e5f80-d8f1-4831-a5d4-51164e6fd945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.029 , 0.025 , 0.0248])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store errors for each split\n",
    "set_error= np.zeros(3)\n",
    "\n",
    "for i in range(3): # repeat 3 times with different splits\n",
    "    default_train, default_valid = train_test_split(Default, test_size=5000, random_state=i)\n",
    "    #preprocess data\n",
    "    X_train= var.fit_transform(default_train)\n",
    "    y_train= default_train[\"default\"] == \"Yes\"\n",
    "    X_valid = var.transform(default_valid)\n",
    "    y_valid= default_valid[\"default\"] == \"Yes\"\n",
    "    #train model\n",
    "    model= sm.GLM(y_train, X_train, family= sm.families.Binomial())\n",
    "    results= model.fit()\n",
    "    #predict on validation set\n",
    "    valid_pred= results.predict(X_valid)\n",
    "    valid_pred_class= (valid_pred>0.5)\n",
    "    #compute validation error\n",
    "    set_error[i]= np.mean(y_valid!=valid_pred_class)\n",
    "set_error    \n",
    "\n",
    "#Comments: The errors are very close indicating that the model's performance is stable across different training-validation splits\n",
    "#No major overfitting - since errors do not spike in any split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b5881f-8521-47ff-84f3-047628d936c1",
   "metadata": {},
   "source": [
    "##### d). Now consider a logistic regression model that predicts the probabilty of default using income, balance and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leand to a reduction in the test error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d90e900c-30d0-4367-91f3-f1ed8bd49ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0254"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropVar = Default.columns.drop([\"default\"])\n",
    "design= MS(dropVar)\n",
    "X_train= design.fit_transform(default_train)\n",
    "y_train= default_train[\"default\"] == \"Yes\"\n",
    "X_valid= design.transform(default_valid)\n",
    "y_valid= default_valid[\"default\"]== \"Yes\"\n",
    "model = sm.GLM(y_train, X_train, family= sm.families.Binomial())\n",
    "results= model.fit()\n",
    "#predict on validation set\n",
    "valid_pred= results.predict(X_valid)\n",
    "valid_pred_class = (valid_pred>0.5)\n",
    "np.mean(y_valid!=valid_pred_class)\n",
    "\n",
    "#Adding the student dummy variable slightly reduces the test error\n",
    "#The improvement is marginal suggesting that student status has a weak but non-zero effect on default prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac6ee9f-d39e-4267-a447-35023c622142",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "##### We continue to consider the use of a logistic regression model to predict the probability of ***default*** using ***income*** and ***balance*** on the ***Default*** data set. In particular, we will now compute estimates for the standard errors of the ***income*** amd ***balance*** logistic regression coefficients in two different ways:\n",
    "##### 1) using the bootstrap and\n",
    "##### 2) using the standard formula for computing the standard errors in the GLM(). Do not forget to set a random seed before beginning your analysis\n",
    "\n",
    "##### ***(a)*** Using the summarize() and sm.GLM() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "f0c9ee98-882f-4094-8126-8423f6d6d01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept    0.435000\n",
       "balance      0.000000\n",
       "income       0.000005\n",
       "Name: std err, dtype: float64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropVar= Default.columns.drop([\"default\", \"student\"])\n",
    "X= MS(dropVar).fit_transform(Default[dropVar])\n",
    "y= Default[\"default\"] == \"Yes\"\n",
    "model= sm.GLM(y, X, family=sm.families.Binomial())\n",
    "results= model.fit()\n",
    "summarize(results)[\"std err\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d77659-6372-4716-8bd8-e05357392abf",
   "metadata": {},
   "source": [
    "##### ***(b)*** Write a function, boot_fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ac715c62-7581-4e69-8c04-f2e26781a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_fn(predictors, response, D, idx):\n",
    "    D_ = D.loc[idx]\n",
    "    Y_ =D_[response]\n",
    "    X_ = clone(predictors).fit_transform(D_)\n",
    "    model= sm.GLM(Y_, X_, family= sm.families.Binomial()).fit()\n",
    "    return model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645edb5a-687e-4999-b1d2-e088c8853444",
   "metadata": {},
   "source": [
    "##### ***(c)*** Following the bootstrap example in the lab, use your boot_fn() function to estimate the standard errors of the logistic regression coefficients for income and balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "590aef4e-1e36-49a4-b6d0-23369d58467e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.16416373e+01,  5.73877605e-03,  1.87775777e-05]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert default from strings to binary\n",
    "Default[\"default\"] = Default[\"default\"].map({\"No\":0, \"Yes\":1})\n",
    "bal_inc_func= partial(boot_fn, MS([\"balance\", \"income\"]), \"default\")\n",
    "rng= np.random.default_rng(0)\n",
    "#get actual indices\n",
    "indices= Default.index.tolist()\n",
    "np.array([bal_inc_func(Default, rng.choice(indices, len(indices), replace=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "4d1deece-fe83-4926-88d3-8a23b36bf0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept SE: 0.4552564131999408\n",
      "Balance SE: 0.00023821330741163237\n",
      "Income SE: 5.0309299677176e-06\n"
     ]
    }
   ],
   "source": [
    "#generate multiple bootstrap samples and collect results\n",
    "B= 1000\n",
    "#initialize boot_coefs as a 2D array to store multiple coeffs assuming there aew 3 coefficients (intercept, balance, income)\n",
    "boot_coefs=np.zeros((B, 3))\n",
    "for i in range(B):\n",
    "    #get bootstrap sample indices\n",
    "    boot_idx = rng.choice(indices, len(indices), replace=True)\n",
    "    #get coeffs\n",
    "    results = bal_inc_func(Default, boot_idx)\n",
    "    #convert to numpy array and store\n",
    "    coeff_array= np.array(results)\n",
    "    boot_coefs[i, :]=coeff_array\n",
    "#calculate standard errors from bootstrap samples\n",
    "SE = boot_coefs.std(axis=0)\n",
    "print(\"Intercept SE:\", SE[0] )\n",
    "print(\"Balance SE:\", SE[1])\n",
    "print(\"Income SE:\", SE[2])\n",
    "\n",
    "#The bootstrap method yields lower SE compared to sm.GLM()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb71aca-a766-4904-9795-f5616cec4219",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "##### We say that ***cross_validate()*** function  can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just sm.GLM() and the predict() method of the fitted model within a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the ***Weekly*** data set. \n",
    "\n",
    "##### ***(a)*** Fit a logistic regression model that predicts ***Direction*** using ***Lag1*** and ***Lag2***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "4997cd79-c73d-446e-9f0f-0c78d2aa7853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>0.2212</td>\n",
       "      <td>0.061</td>\n",
       "      <td>3.599</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag1</th>\n",
       "      <td>-0.0387</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-1.477</td>\n",
       "      <td>0.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag2</th>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.027</td>\n",
       "      <td>2.270</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             coef  std err      z  P>|z|\n",
       "intercept  0.2212    0.061  3.599  0.000\n",
       "Lag1      -0.0387    0.026 -1.477  0.140\n",
       "Lag2       0.0602    0.027  2.270  0.023"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weekly = load_data(\"Weekly\")\n",
    "#Weekly.Direction.values\n",
    "dropVar= Weekly.columns.drop([\"Year\", \"Lag3\", \"Lag4\", \"Lag5\", \"Volume\", \"Today\", \"Direction\"])\n",
    "Weekly[\"Direction_binary\"] = Weekly[\"Direction\"].map({\"Down\":0, \"Up\":1})\n",
    "X= MS(dropVar).fit_transform(Weekly)\n",
    "y= Weekly[\"Direction_binary\"]\n",
    "model= sm.GLM(y, X, family= sm.families.Binomial())\n",
    "results= model.fit()\n",
    "summarize(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c37483-957a-4f6e-92c0-c5fe28d41474",
   "metadata": {},
   "source": [
    "##### ***(b)*** Fit a logistic regression model that predicts Direction using Lag1 and Lag2 using all but the first observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "7c8e6044-90d8-4116-b9ab-b4468a58a285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>0.2232</td>\n",
       "      <td>0.061</td>\n",
       "      <td>3.630</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag1</th>\n",
       "      <td>-0.0384</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-1.466</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lag2</th>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.027</td>\n",
       "      <td>2.291</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             coef  std err      z  P>|z|\n",
       "intercept  0.2232    0.061  3.630  0.000\n",
       "Lag1      -0.0384    0.026 -1.466  0.143\n",
       "Lag2       0.0608    0.027  2.291  0.022"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get all observations except the first one\n",
    "X_loocv= X.iloc[1:]\n",
    "y_loocv = y.iloc[1:]\n",
    "#fit the model on all but first observation\n",
    "model_loocv= sm.GLM(y_loocv, X_loocv, family= sm.families.Binomial())\n",
    "results_loocv= model_loocv.fit()\n",
    "summarize(results_loocv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc2ec0-f710-49a9-81e9-2036e9487d94",
   "metadata": {},
   "source": [
    "##### ***(c)*** Use the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if ***P(Direction= \"Up\"|Lag1,Lag2)>0.5***. Was this observation correctly classified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "7f7f693c-6d56-4eea-97b6-40575cf78338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict the left-out observation\n",
    "first_obs= X.iloc[[0]]\n",
    "pred_prob= results_loocv.predict(first_obs)\n",
    "pred_class= 1 if pred_prob[0] > 0.5 else 0\n",
    "#compare with actual class\n",
    "actual_class = y.iloc[0]\n",
    "correct = pred_class==actual_class\n",
    "correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577c33c-9813-4b9c-81ef-a2e58551aeec",
   "metadata": {},
   "source": [
    "##### ***(d)*** Write a for loop from i=1 to i=n, where n is the number of observations in the data set, that performs each of the following steps:\n",
    "\n",
    "##### ***i.*** Fit a logistic regression model using all but the ***ith*** observation to predict Direction using Lag1 and Lag2\n",
    "##### ***ii.*** Compute the posterior probability of the market moving up for the ***ith*** observation.\n",
    "##### ***iii*** Use the posterior probability for the ***ith*** observation in order to predict whether or not the market moves up.\n",
    "##### ***iv*** Determine whether or not an error was made in predicting the direction for the ***ith*** observation. If an error was made, then indicate this as 1, and otherwise indicate it as 0.\n",
    "\n",
    "##### ***(e)*** Take the average of the n numbers obtained in (d)iv in order to obtain the LOOCV estimate for the test error. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "3ccd1ea7-80c5-4ffc-b665-9e7690e6387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOOCV error rate: 0.4490 (489 errors out of 1089 predictions)\n"
     ]
    }
   ],
   "source": [
    "#initialize error counter\n",
    "errors=0\n",
    "n= len(Weekly)\n",
    "for i in range(1, n):\n",
    "    #create training data(all but ith observation)\n",
    "    X_train= X.drop(i)\n",
    "    y_train= y.drop(i)\n",
    "    #fit log regression\n",
    "    model= sm.GLM(y_train, X_train, family= sm.families.Binomial())\n",
    "    result = model.fit()  # Need to fit the model first\n",
    "\n",
    "    #predict left_out observation\n",
    "    X_test= X.iloc[[i]]\n",
    "    pred_prob= result.predict(X_test)  # Use the fitted model result to predict\n",
    "    pred_class= 1 if pred_prob.iloc[0] > 0.5 else 0\n",
    "\n",
    "    #compare with actual\n",
    "    actual = y.iloc[i]\n",
    "    if pred_class != actual:\n",
    "        errors +=1\n",
    "#Calculate LOOCV error rate     \n",
    "loocv_error= errors / n\n",
    "print(f\"LOOCV error rate: {loocv_error:.4f} ({errors} errors out of {n} predictions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d232ff65-e2fa-470e-b307-611343d24b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
